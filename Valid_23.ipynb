{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/NewEncodings/blob/main/Valid_23.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "HTftvdBXRhTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091a0704-f46b-41b1-aaa5-6e75970749f4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.18.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "id": "dPOyq7dZbgNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2da2c995-1fbf-4080-b697-b3bf644a5a11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fused Validation"
      ],
      "metadata": {
        "id": "J1pXJJwyJOdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/MMLearning/data/imdb/rebuttal/47waladata/test\"\n",
        "labels_file = \"/content/MMLearning/data/imdb/rebuttal/47waladata/fused_test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if not (filename.endswith(\"_1.png\") or filename.endswith(\"_2.png\")):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            image_file_paths.append(image_path)\n",
        "            genre_labels.append(labels)\n"
      ],
      "metadata": {
        "id": "1CwXZ1O2JP1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths)"
      ],
      "metadata": {
        "id": "PQAHSUj7KJtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " image_file_paths[0:10]"
      ],
      "metadata": {
        "id": "lHtqNENb9Kjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 100% Missing Modality"
      ],
      "metadata": {
        "id": "p189sy67JxGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _3.png is for Encoded Image\n",
        "# _4.png is for Actual Image"
      ],
      "metadata": {
        "id": "YgODJ0JYLKCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/MMLearning/data/imdb/mulitmodal/test\"\n",
        "labels_file = \"/content/MMLearning/data/imdb/mulitmodal/test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            image_file_paths.append(image_path)\n",
        "            genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "ynpL2klYJyAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths)"
      ],
      "metadata": {
        "id": "XwSGQ4KyKHuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:10]"
      ],
      "metadata": {
        "id": "qV0yKN36cYOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Missing Modality"
      ],
      "metadata": {
        "id": "oJ2_570TOmQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Fused"
      ],
      "metadata": {
        "id": "ZmVu3IKXPGL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "fused_image_file_paths = []\n",
        "fused_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/fused/test\"\n",
        "labels_file = \"/content/fused_test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if not (filename.endswith(\"_1.png\") or filename.endswith(\"_2.png\")):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            fused_image_file_paths.append(image_path)\n",
        "            fused_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "q6_JzJy3Oqkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(fused_image_file_paths)"
      ],
      "metadata": {
        "id": "0sieP3GiPEl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KANHEn8iPVh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Image Only"
      ],
      "metadata": {
        "id": "POjvuFIwPc_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _3.png is for Encoded Image\n",
        "# _4.png is for Actual Image"
      ],
      "metadata": {
        "id": "ftkoQMJnd9ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "img_file_paths = []\n",
        "img_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/joint/test\"\n",
        "labels_file = \"/content/joint_test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "            image_path = os.path.join(image_folder_add, filename)\n",
        "            img_file_paths.append(image_path)\n",
        "            img_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "cwtJhz39Pf4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_file_paths)"
      ],
      "metadata": {
        "id": "DVMBX3XNPpGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBuRK3ehPrfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Selection"
      ],
      "metadata": {
        "id": "jrsK-0C-Sm1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "percentage = 0.5 # Fused Availability"
      ],
      "metadata": {
        "id": "wbE3eNhwSxqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "total_samples = len(fused_image_file_paths)\n",
        "num_samples_from_fused = int(percentage * total_samples)\n",
        "num_samples_from_image = total_samples - num_samples_from_fused"
      ],
      "metadata": {
        "id": "CMe3-wp9Sq05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total_samples\", total_samples)\n",
        "print(\"num_samples_from_fused\", num_samples_from_fused)\n",
        "print(\"num_samples_from_image\", num_samples_from_image)"
      ],
      "metadata": {
        "id": "Igu7VBdDeW11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_indices_fused = random.sample(range(total_samples), num_samples_from_fused)\n",
        "remaining_indices_image = [i for i in range(total_samples) if i not in random_indices_fused]\n",
        "\n",
        "\n",
        "image_file_paths = [fused_image_file_paths[i] for i in random_indices_fused] + [img_file_paths[i] for i in remaining_indices_image]\n",
        "genre_labels = [fused_genre_labels[i] for i in random_indices_fused] + [img_genre_labels[i] for i in remaining_indices_image]"
      ],
      "metadata": {
        "id": "lE26f9GOTRiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[7000]"
      ],
      "metadata": {
        "id": "LhIGoUJkejq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_labels[7000]"
      ],
      "metadata": {
        "id": "yqhgcI8oenBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "1w1c6ddCKg7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1"
      ],
      "metadata": {
        "id": "TgQgz3cBKhW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "id": "X1NyQ4jVKiZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 20\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "jjIjX7twKsOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels, len(valid_labels)"
      ],
      "metadata": {
        "id": "WJldcbI5Kvyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHot Encoding"
      ],
      "metadata": {
        "id": "aVh_shMHK0sv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_hot_labels = []\n",
        "\n",
        "for labels in genre_labels:\n",
        "    multi_hot = [1 if label in labels else 0 for label in valid_labels]\n",
        "    multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "ndkQ-dBgK28s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "multi_hot_labels[1]"
      ],
      "metadata": {
        "id": "xqCyzIerK3d4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_labels[1]"
      ],
      "metadata": {
        "id": "13t0HM8fK5iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset and Validation (ResNet Fused)"
      ],
      "metadata": {
        "id": "wmwBygvfLZri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomMultiLabelDataset(Dataset):\n",
        "    def __init__(self, image_file_paths, multi_encoded_labels, transform=None):\n",
        "        self.image_file_paths = image_file_paths\n",
        "        self.multi_encoded_labels = multi_encoded_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_file_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        labels = self.multi_encoded_labels[idx]\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "# Define data transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "jP04dLeHLaFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset\n",
        "val_dataset = CustomMultiLabelDataset(image_file_paths, multi_hot_labels, transform=transform)\n",
        "print(len(val_dataset))"
      ],
      "metadata": {
        "id": "R7LEJLN0Layt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model and Validation Loop (ResNet)"
      ],
      "metadata": {
        "id": "zNXm21P2lKQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "save_dir = '/content/MMLearning/data/Models/Model-07'\n",
        "load_path = os.path.join(save_dir, '14_model.pth')\n",
        "\n",
        "model = models.resnet50(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(valid_labels))\n",
        "model.load_state_dict(torch.load(load_path))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "SsIJ_MdjL1la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "153lIoMpLpVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "img_predictions = []\n",
        "true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # img_predictions.extend((outputs > 0.5).int().cpu().numpy())\n",
        "        img_predictions.extend(F.sigmoid(outputs).cpu().numpy())\n",
        "        true_labels.extend(labels.int().cpu().numpy())"
      ],
      "metadata": {
        "id": "V1C8zu8kMxS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "img_predictions = np.array(img_predictions)"
      ],
      "metadata": {
        "id": "tGwZwklm3-GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.3\n",
        "predictions = (img_predictions >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "nUCgUl0s2U02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "precision = precision_score(true_labels, predictions, average='macro')\n",
        "recall = recall_score(true_labels, predictions, average='macro')\n",
        "f1 = f1_score(true_labels, predictions, average='macro')"
      ],
      "metadata": {
        "id": "Qo3yv027NCUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(f\"Precision: {precision}\")\n",
        "# print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "xzGxqHKwNNlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(true_labels, predictions))"
      ],
      "metadata": {
        "id": "K_tSuJ3_NQDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset and Validation (ViT Fused)"
      ],
      "metadata": {
        "id": "OnDO1DCiiyIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "LwYd4-YniyoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "val_data = {'image': image_file_paths, 'label': multi_hot_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "PdyuGkvXi_Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val"
      ],
      "metadata": {
        "id": "wS_rL2WhjBwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_val['image'][0]"
      ],
      "metadata": {
        "id": "J-4CM-Q6jFGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = valid_labels\n",
        "labels"
      ],
      "metadata": {
        "id": "xHMrTOFWjFPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model"
      ],
      "metadata": {
        "id": "CB0GazEujdFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTForImageClassification.from_pretrained(f\"/content/MMLearning/data/Models/Model-05/checkpoint-19999\").to(device)\n",
        "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
      ],
      "metadata": {
        "id": "IYmACS61jFWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "iVpwGu5UjFcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "X5-TuOYFjmPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "aDYXLb8EjpnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = val_dataset[0]"
      ],
      "metadata": {
        "id": "h_I6n33djr79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "id": "ozGHp_XpjuTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "kSp1wDWdju1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "kumR8n8sj0F7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "valid_dataset_loader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Gwq7UBrJj5nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "vit_predictions, labels = [], []\n",
        "\n",
        "for batch in valid_dataset_loader:\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    label_ids = batch[\"labels\"].to(device).float()\n",
        "\n",
        "    outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "    logits = outputs.logits.detach().cpu()\n",
        "\n",
        "    # predictions.extend((logits > 0.5).int().cpu().numpy())\n",
        "    vit_predictions.extend(F.sigmoid(logits).cpu().numpy())\n",
        "    labels.extend(label_ids.int().cpu().numpy())"
      ],
      "metadata": {
        "id": "jF-KBkj7kAWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "vit_predictions = np.array(vit_predictions)"
      ],
      "metadata": {
        "id": "aLrAVKuH43_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "predictions = (vit_predictions >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "9_19ai3q5Gr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "# accuracy = accuracy_score(labels, predictions)\n",
        "# precision = precision_score(labels, predictions, average='macro')\n",
        "# recall = recall_score(labels, predictions, average='macro')\n",
        "f1 = f1_score(labels, predictions, average='macro')"
      ],
      "metadata": {
        "id": "Us9pLc9pkBFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(f\"Precision: {precision}\")\n",
        "# print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "LnW_vy4XkI9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(labels, predictions))"
      ],
      "metadata": {
        "id": "tBJ2m3ikkKpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FeTzRkTzZgOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PreProcessing for Joint Missing Modality"
      ],
      "metadata": {
        "id": "tl5A8iglZg_f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# _3.png is for Encoded Image\n",
        "# _4.png is for Actual Image"
      ],
      "metadata": {
        "id": "2RQBaqAvZpGI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "genre_labels = []\n",
        "encoded_file_paths = []\n",
        "encoded_genre_labels = []\n",
        "\n",
        "image_folder_add = \"/content/MMLearning/data/imdb/mulitmodal/test\"\n",
        "labels_file = \"/content/MMLearning/data/imdb/mulitmodal/test_label.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split('|')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip().split(', ')  # Split labels by comma and remove leading/trailing spaces\n",
        "        image_path = os.path.join(image_folder_add, filename)\n",
        "\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "            image_file_paths.append(image_path)\n",
        "            genre_labels.append(labels)\n",
        "\n",
        "        if filename.endswith(\"_3.png\"):\n",
        "            encoded_file_paths.append(image_path)\n",
        "            encoded_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "2_igZSwAZihl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths)"
      ],
      "metadata": {
        "id": "UrEG_UNSZmqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66ef8d2-7046-47ee-f241-f0c8e5aa624f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7799"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(encoded_file_paths)"
      ],
      "metadata": {
        "id": "2hlashn4ajX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e771a4-1cce-4294-d699-338bfcfae5d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7799"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5]"
      ],
      "metadata": {
        "id": "aKPaZlNWCARj",
        "outputId": "a167c8ae-ac58-4031-bf76-6e09706032fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/MMLearning/data/imdb/mulitmodal/test/0078718_4.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0089003_4.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0098136_4.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0057693_4.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0385330_4.png']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_file_paths[0:5]"
      ],
      "metadata": {
        "id": "UbHKmY47CEII",
        "outputId": "a2f065ac-eac0-4153-d4fc-c6ab166e2220",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/MMLearning/data/imdb/mulitmodal/test/0078718_3.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0089003_3.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0098136_3.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0057693_3.png',\n",
              " '/content/MMLearning/data/imdb/mulitmodal/test/0385330_3.png']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "\n",
        "for labels in genre_labels:\n",
        "    for label in labels:\n",
        "        label_counts[label] += 1"
      ],
      "metadata": {
        "id": "K_ciS54kZ2FZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_count_list = [(label, count) for label, count in label_counts.items()]\n",
        "sorted_label_count_list = sorted(label_count_list, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for label, count in sorted_label_count_list:\n",
        "    print(f\"{label}: {count}\")\n",
        "\n",
        "print(\"Total Labels: \", len(label_count_list))"
      ],
      "metadata": {
        "id": "tVB92IOVZ3FN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455b8608-0cfb-41ca-b857-a015d026586f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drama: 4142\n",
            "Comedy: 2611\n",
            "Romance: 1590\n",
            "Thriller: 1567\n",
            "Crime: 1163\n",
            "Action: 1044\n",
            "Horror: 825\n",
            "Adventure: 821\n",
            "Documentary: 629\n",
            "Mystery: 617\n",
            "Sci-Fi: 586\n",
            "Fantasy: 585\n",
            "Family: 518\n",
            "Biography: 411\n",
            "War: 401\n",
            "History: 345\n",
            "Music: 311\n",
            "Animation: 306\n",
            "Musical: 253\n",
            "Western: 210\n",
            "Sport: 191\n",
            "Short: 142\n",
            "Film-Noir: 102\n",
            "News: 19\n",
            "Adult: 1\n",
            "Total Labels:  25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_label_count = 20\n",
        "valid_labels = [label for label, count in label_counts.items() if count >= min_label_count]\n",
        "valid_labels = sorted(list(valid_labels))"
      ],
      "metadata": {
        "id": "ELjNbwf7Z6a6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels, len(valid_labels)"
      ],
      "metadata": {
        "id": "oC7agfy1Z9lI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ac325fa-80af-40ab-a9ef-f53122025f96"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Action',\n",
              "  'Adventure',\n",
              "  'Animation',\n",
              "  'Biography',\n",
              "  'Comedy',\n",
              "  'Crime',\n",
              "  'Documentary',\n",
              "  'Drama',\n",
              "  'Family',\n",
              "  'Fantasy',\n",
              "  'Film-Noir',\n",
              "  'History',\n",
              "  'Horror',\n",
              "  'Music',\n",
              "  'Musical',\n",
              "  'Mystery',\n",
              "  'Romance',\n",
              "  'Sci-Fi',\n",
              "  'Short',\n",
              "  'Sport',\n",
              "  'Thriller',\n",
              "  'War',\n",
              "  'Western'],\n",
              " 23)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_of_classes = len(valid_labels)"
      ],
      "metadata": {
        "id": "vL2S95OAuNr1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi Hot Encoding"
      ],
      "metadata": {
        "id": "2IFOfBBIasY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_multi_hot_labels = []\n",
        "\n",
        "for labels in genre_labels:\n",
        "    multi_hot = [1 if label in labels else 0 for label in valid_labels]\n",
        "    img_multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "QTUb7FmIaxOd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_multi_hot_labels = []\n",
        "\n",
        "for labels in encoded_genre_labels:\n",
        "    multi_hot = [1 if label in labels else 0 for label in valid_labels]\n",
        "    enc_multi_hot_labels.append(multi_hot)"
      ],
      "metadata": {
        "id": "b-hwrS0Va1qT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Check"
      ],
      "metadata": {
        "id": "Cbtn0VatbEj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 10"
      ],
      "metadata": {
        "id": "YVTl-eowMbHD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[index], genre_labels[index] , img_multi_hot_labels[index]"
      ],
      "metadata": {
        "id": "2aybS0WIbDxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13818236-d56d-4920-e1e0-6f43b60a167c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/MMLearning/data/imdb/mulitmodal/test/0023369_4.png',\n",
              " ['Drama'],\n",
              " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_file_paths[index], encoded_genre_labels[index] , enc_multi_hot_labels[index]"
      ],
      "metadata": {
        "id": "MHkQhEBgbCrZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1447341-e00a-49f7-8e04-fac97a2e36b9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/MMLearning/data/imdb/mulitmodal/test/0023369_3.png',\n",
              " ['Drama'],\n",
              " [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Missing Modality with ResNet (Joint)"
      ],
      "metadata": {
        "id": "Nd4-m70_rToh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomMultiLabelDataset(Dataset):\n",
        "    def __init__(self, image_file_paths, multi_encoded_labels, transform=None):\n",
        "        self.image_file_paths = image_file_paths\n",
        "        self.multi_encoded_labels = multi_encoded_labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_file_paths[idx]\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        labels = self.multi_encoded_labels[idx]\n",
        "        labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "# Define data transformations (resize, normalize, etc.)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "HNHkrD-SbbAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset\n",
        "img_dataset = CustomMultiLabelDataset(image_file_paths, img_multi_hot_labels, transform=transform)\n",
        "enc_dataset = CustomMultiLabelDataset(encoded_file_paths, enc_multi_hot_labels, transform=transform)"
      ],
      "metadata": {
        "id": "_ChZ3NhqbfGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(img_dataset) , len(enc_dataset)"
      ],
      "metadata": {
        "id": "FJvAcaV6bkaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_loader = DataLoader(img_dataset, batch_size=batch_size, shuffle=False)\n",
        "enc_loader = DataLoader(enc_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "ok1w5X-6b5DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(enc_loader)\n",
        "num_batches_with_logits_3 = int(num_batches * 0.3)  # ___% of batches that you want to include (30 for now)\n",
        "print(\"Total Batches: \", num_batches)\n",
        "print(\"Missing Modaility Batches: \", num_batches_with_logits_3)"
      ],
      "metadata": {
        "id": "8urLJDx4piCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "controller = random.sample(range(1, num_batches+1), num_batches_with_logits_3)\n",
        "controller = sorted(controller)"
      ],
      "metadata": {
        "id": "otk7lM1YZDlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(controller)"
      ],
      "metadata": {
        "id": "_GrRclwkZQYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "import os\n",
        "\n",
        "save_dir = '/content/Model/Models-Train-22/'\n",
        "load_path = os.path.join(save_dir, '29_model.pth')\n",
        "\n",
        "model = models.resnet101(pretrained=False)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(valid_labels))\n",
        "model.load_state_dict(torch.load(load_path))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "U1rtaBPscK4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "img_predictions = []\n",
        "img_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in img_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        img_predictions.extend(F.sigmoid(outputs).cpu().numpy())\n",
        "        img_true_labels.extend(labels.int().cpu().numpy())"
      ],
      "metadata": {
        "id": "e7FdRFn8cRDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_predictions = []\n",
        "enc_true_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_idx = 0\n",
        "    for inputs, labels in enc_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        if batch_idx in controller:\n",
        "            outputs = model(inputs)\n",
        "        else:\n",
        "            length = len(labels)\n",
        "            outputs = torch.zeros((length, num_of_classes), device=\"cpu\")\n",
        "\n",
        "        enc_predictions.extend(F.sigmoid(outputs).cpu().numpy())\n",
        "        enc_true_labels.extend(labels.int().cpu().numpy())\n",
        "        batch_idx = batch_idx + 1"
      ],
      "metadata": {
        "id": "mpKTF3M7ckSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predicted_avg = (np.array(img_predictions) + np.array(enc_predictions)) / 2.0"
      ],
      "metadata": {
        "id": "aUOQ9EeucsxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the threshold of 0.5\n",
        "threshold = 0.5\n",
        "predictions = (predicted_avg > threshold).astype(int)"
      ],
      "metadata": {
        "id": "eTdiU9Gkcxjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = img_true_labels"
      ],
      "metadata": {
        "id": "aTnj8fYKL0W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "precision = precision_score(true_labels, predictions, average='macro')\n",
        "recall = recall_score(true_labels, predictions, average='macro')\n",
        "f1 = f1_score(true_labels, predictions, average='macro')"
      ],
      "metadata": {
        "id": "qDXnmSGNdB5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "ABUg7EcqdCY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(true_labels, predictions))"
      ],
      "metadata": {
        "id": "-MGGWjSLdGhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partial Missing Modality with ViT (Joint)"
      ],
      "metadata": {
        "id": "M-bIORJbrF0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "8hUFQbz1rGMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00b7ecf3-f04d-47ec-bd17-e2c2e31b1b3a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "img_data = {'image': image_file_paths, 'label': img_multi_hot_labels}\n",
        "ds_img = Dataset.from_dict(img_data)\n",
        "\n",
        "enc_data = {'image': encoded_file_paths, 'label': enc_multi_hot_labels}\n",
        "ds_enc = Dataset.from_dict(enc_data)"
      ],
      "metadata": {
        "id": "HIRPK-KDro34"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "BvkSQkFKr7xw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_dataset = ds_img.with_transform(transform)\n",
        "enc_dataset = ds_enc.with_transform(transform)"
      ],
      "metadata": {
        "id": "UvSRzA_UrgAN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "-bmvjU5ysCDk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "img_loader = DataLoader(img_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)\n",
        "enc_loader = DataLoader(enc_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "Ofb2m7WbsRFU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(enc_loader)\n",
        "num_batches_with_logits_3 = int(num_batches * 0.9)  # ___% of batches that you want to include (30 for now)\n",
        "print(\"Total Batches: \", num_batches)\n",
        "print(\"Missing Modaility Batches: \", num_batches_with_logits_3)"
      ],
      "metadata": {
        "id": "hi16SQ8esRVM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e936e465-94a9-4de8-e135-fb6a9241d273"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Batches:  244\n",
            "Missing Modaility Batches:  219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "controller = random.sample(range(1, num_batches+1), num_batches_with_logits_3)\n",
        "controller = sorted(controller)"
      ],
      "metadata": {
        "id": "Bs-9K817ZdjT"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(controller)"
      ],
      "metadata": {
        "id": "XsNA95CaZd_7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8181128a-4165-4671-ecf3-82eaae8a4712"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Model"
      ],
      "metadata": {
        "id": "zpKCkTaBso_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ViTForImageClassification.from_pretrained(f\"/content/MMLearning/data/Models/Model-05/checkpoint-19999\").to(device)\n",
        "image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
      ],
      "metadata": {
        "id": "2hRhY0VprHzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85864bd9-208c-42db-b252-63240f34075e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file /content/MMLearning/data/Models/Model-05/checkpoint-19999/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"Action\",\n",
            "    \"1\": \"Adventure\",\n",
            "    \"2\": \"Animation\",\n",
            "    \"3\": \"Biography\",\n",
            "    \"4\": \"Comedy\",\n",
            "    \"5\": \"Crime\",\n",
            "    \"6\": \"Documentary\",\n",
            "    \"7\": \"Drama\",\n",
            "    \"8\": \"Family\",\n",
            "    \"9\": \"Fantasy\",\n",
            "    \"10\": \"Film-Noir\",\n",
            "    \"11\": \"History\",\n",
            "    \"12\": \"Horror\",\n",
            "    \"13\": \"Music\",\n",
            "    \"14\": \"Musical\",\n",
            "    \"15\": \"Mystery\",\n",
            "    \"16\": \"Romance\",\n",
            "    \"17\": \"Sci-Fi\",\n",
            "    \"18\": \"Short\",\n",
            "    \"19\": \"Sport\",\n",
            "    \"20\": \"Thriller\",\n",
            "    \"21\": \"War\",\n",
            "    \"22\": \"Western\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Action\": \"0\",\n",
            "    \"Adventure\": \"1\",\n",
            "    \"Animation\": \"2\",\n",
            "    \"Biography\": \"3\",\n",
            "    \"Comedy\": \"4\",\n",
            "    \"Crime\": \"5\",\n",
            "    \"Documentary\": \"6\",\n",
            "    \"Drama\": \"7\",\n",
            "    \"Family\": \"8\",\n",
            "    \"Fantasy\": \"9\",\n",
            "    \"Film-Noir\": \"10\",\n",
            "    \"History\": \"11\",\n",
            "    \"Horror\": \"12\",\n",
            "    \"Music\": \"13\",\n",
            "    \"Musical\": \"14\",\n",
            "    \"Mystery\": \"15\",\n",
            "    \"Romance\": \"16\",\n",
            "    \"Sci-Fi\": \"17\",\n",
            "    \"Short\": \"18\",\n",
            "    \"Sport\": \"19\",\n",
            "    \"Thriller\": \"20\",\n",
            "    \"War\": \"21\",\n",
            "    \"Western\": \"22\"\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"problem_type\": \"multi_label_classification\",\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.32.1\"\n",
            "}\n",
            "\n",
            "loading weights file /content/MMLearning/data/Models/Model-05/checkpoint-19999/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "All the weights of ViTForImageClassification were initialized from the model checkpoint at /content/MMLearning/data/Models/Model-05/checkpoint-19999.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTForImageClassification for predictions without further training.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/preprocessor_config.json\n",
            "size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n",
            "Image processor ViTImageProcessor {\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"image_processor_type\": \"ViTImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.5,\n",
            "    0.5,\n",
            "    0.5\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 224,\n",
            "    \"width\": 224\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "img_predictions = []\n",
        "img_true_labels = []\n",
        "\n",
        "for batch in img_loader:\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    label_ids = batch[\"labels\"].to(device).float()\n",
        "\n",
        "    outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "\n",
        "    logits = outputs.logits.detach().cpu()\n",
        "    # probabilities = torch.softmax(logits, dim=1)\n",
        "\n",
        "    img_predictions.extend(F.sigmoid(logits).cpu().numpy())\n",
        "    img_true_labels.extend(label_ids.int().cpu().numpy())"
      ],
      "metadata": {
        "id": "yqOokruysqVc"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_predictions = []\n",
        "enc_true_labels = []\n",
        "\n",
        "batch_idx = 0\n",
        "for batch in enc_loader:\n",
        "    pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    label_ids = batch[\"labels\"].to(device).float()\n",
        "\n",
        "    if batch_idx in controller:\n",
        "        outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "        logits = outputs.logits.detach().cpu()\n",
        "    else:\n",
        "        length = len(label_ids)\n",
        "        logits = torch.zeros((length, num_of_classes), device=\"cpu\")\n",
        "\n",
        "    # probabilities = torch.softmax(logits, dim=1)\n",
        "    enc_predictions.extend(F.sigmoid(logits).cpu().numpy())\n",
        "    enc_true_labels.extend(label_ids.int().cpu().numpy())\n",
        "    batch_idx = batch_idx + 1"
      ],
      "metadata": {
        "id": "GbmPtCSesv4C"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "threshold = 0.5\n",
        "img_predictions = (np.array(img_predictions) >= threshold).astype(int)\n",
        "enc_predictions = (np.array(enc_predictions) >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "suKQct4u7IRE"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predicted_avg = (np.array(img_predictions) + np.array(enc_predictions)) / 2.0"
      ],
      "metadata": {
        "id": "GqMeprz7szcn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the threshold of 0.5\n",
        "threshold = 0.5\n",
        "predictions = (predicted_avg >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "Ntfc13DEszy-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicted_avg[10:15]"
      ],
      "metadata": {
        "id": "oucHf_Fl8lnR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions[10:15]"
      ],
      "metadata": {
        "id": "F_vpgtch-LrC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = img_true_labels"
      ],
      "metadata": {
        "id": "AqMG5gMfvSIY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# accuracy = accuracy_score(true_labels, predictions)\n",
        "# precision = precision_score(true_labels, predictions, average='macro')\n",
        "# recall = recall_score(true_labels, predictions, average='macro')\n",
        "f1 = f1_score(true_labels, predictions, average='macro')"
      ],
      "metadata": {
        "id": "FVnkU1Yps2Fk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"Accuracy: {accuracy}\")\n",
        "# print(f\"Precision: {precision}\")\n",
        "# print(f\"Recall: {recall}\")\n",
        "print(f\"F1-Score: {f1}\")"
      ],
      "metadata": {
        "id": "oYLi4SJhs4el",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ef8e57-b585-44da-b79a-f472b4949074"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1-Score: 0.3162483818802372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(true_labels, predictions))"
      ],
      "metadata": {
        "id": "z6d7QJ7ws6ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496e4384-22a6-45f6-d373-98a7825ed16a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.65      0.46      1044\n",
            "           1       0.26      0.46      0.33       821\n",
            "           2       0.18      0.62      0.28       306\n",
            "           3       0.11      0.28      0.16       411\n",
            "           4       0.49      0.80      0.61      2611\n",
            "           5       0.35      0.61      0.44      1163\n",
            "           6       0.31      0.73      0.43       629\n",
            "           7       0.62      0.89      0.73      4142\n",
            "           8       0.22      0.53      0.31       518\n",
            "           9       0.19      0.45      0.27       585\n",
            "          10       0.02      0.16      0.03       102\n",
            "          11       0.10      0.28      0.14       345\n",
            "          12       0.35      0.75      0.47       825\n",
            "          13       0.13      0.43      0.20       311\n",
            "          14       0.06      0.21      0.09       253\n",
            "          15       0.20      0.44      0.27       617\n",
            "          16       0.36      0.57      0.44      1590\n",
            "          17       0.27      0.66      0.39       586\n",
            "          18       0.04      0.26      0.07       142\n",
            "          19       0.08      0.38      0.13       191\n",
            "          20       0.40      0.70      0.51      1567\n",
            "          21       0.20      0.56      0.30       401\n",
            "          22       0.12      0.53      0.19       210\n",
            "\n",
            "   micro avg       0.33      0.66      0.44     19370\n",
            "   macro avg       0.24      0.52      0.32     19370\n",
            "weighted avg       0.38      0.66      0.48     19370\n",
            " samples avg       0.52      0.71      0.53     19370\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_labels"
      ],
      "metadata": {
        "id": "3kJdHLhvEQV6",
        "outputId": "2257f6e9-64be-41d8-e895-86fe760f3d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Action',\n",
              " 'Adventure',\n",
              " 'Animation',\n",
              " 'Biography',\n",
              " 'Comedy',\n",
              " 'Crime',\n",
              " 'Documentary',\n",
              " 'Drama',\n",
              " 'Family',\n",
              " 'Fantasy',\n",
              " 'Film-Noir',\n",
              " 'History',\n",
              " 'Horror',\n",
              " 'Music',\n",
              " 'Musical',\n",
              " 'Mystery',\n",
              " 'Romance',\n",
              " 'Sci-Fi',\n",
              " 'Short',\n",
              " 'Sport',\n",
              " 'Thriller',\n",
              " 'War',\n",
              " 'Western']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "true_labels = np.array(true_labels, dtype=int)\n",
        "predictions = np.array(predictions, dtype=int)\n",
        "print(multilabel_confusion_matrix(true_labels, predictions, labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]))\n",
        "# print(multilabel_confusion_matrix(true_labels, predictions))"
      ],
      "metadata": {
        "id": "D3lXrWbdC7ED",
        "outputId": "39e5135a-d02e-4afd-c7ca-67c1cf884d94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[5528 1227]\n",
            "  [ 367  677]]\n",
            "\n",
            " [[5924 1054]\n",
            "  [ 446  375]]\n",
            "\n",
            " [[6604  889]\n",
            "  [ 115  191]]\n",
            "\n",
            " [[6484  904]\n",
            "  [ 297  114]]\n",
            "\n",
            " [[3057 2131]\n",
            "  [ 529 2082]]\n",
            "\n",
            " [[5325 1311]\n",
            "  [ 458  705]]\n",
            "\n",
            " [[6141 1029]\n",
            "  [ 172  457]]\n",
            "\n",
            " [[1426 2231]\n",
            "  [ 467 3675]]\n",
            "\n",
            " [[6343  938]\n",
            "  [ 246  272]]\n",
            "\n",
            " [[6130 1084]\n",
            "  [ 323  262]]\n",
            "\n",
            " [[6870  827]\n",
            "  [  86   16]]\n",
            "\n",
            " [[6542  912]\n",
            "  [ 248   97]]\n",
            "\n",
            " [[5797 1177]\n",
            "  [ 203  622]]\n",
            "\n",
            " [[6617  871]\n",
            "  [ 177  134]]\n",
            "\n",
            " [[6675  871]\n",
            "  [ 201   52]]\n",
            "\n",
            " [[6075 1107]\n",
            "  [ 346  271]]\n",
            "\n",
            " [[4569 1640]\n",
            "  [ 679  911]]\n",
            "\n",
            " [[6171 1042]\n",
            "  [ 197  389]]\n",
            "\n",
            " [[6832  825]\n",
            "  [ 105   37]]\n",
            "\n",
            " [[6776  832]\n",
            "  [ 119   72]]\n",
            "\n",
            " [[4589 1643]\n",
            "  [ 472 1095]]\n",
            "\n",
            " [[6501  897]\n",
            "  [ 176  225]]\n",
            "\n",
            " [[6746  843]\n",
            "  [  98  112]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "be5vw4_EE3uJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WpvzS4jF96d"
      },
      "execution_count": 41,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "J1pXJJwyJOdp",
        "p189sy67JxGD",
        "oJ2_570TOmQZ",
        "1w1c6ddCKg7z",
        "wmwBygvfLZri",
        "OnDO1DCiiyIY",
        "tl5A8iglZg_f",
        "Nd4-m70_rToh",
        "M-bIORJbrF0c"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}