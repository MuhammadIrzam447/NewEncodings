{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NzBfAyc7apgv",
        "LWIDbmsuZ6yr"
      ],
      "authorship_tag": "ABX9TyPkOEMakc3nQu7kmAk4KZeF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/NewEncodings/blob/main/Train_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Notebook to Train and Evaluate ResNet and ViT on 100 and __% datasets residing in a single directory."
      ],
      "metadata": {
        "id": "VxazLvh8JMJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data\n"
      ],
      "metadata": {
        "id": "jNTJQ9CCMOuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# # Define the path to your dataset folder\n",
        "# dataset_folder = '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test'\n",
        "\n",
        "# # Define the path for the output text file\n",
        "# output_file = '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test_dataset.txt'\n",
        "\n",
        "# # Open the output file for writing\n",
        "# with open(output_file, 'w') as f:\n",
        "#     # Iterate through the subdirectories in the dataset folder\n",
        "#     for label in os.listdir(dataset_folder):\n",
        "#         label_path = os.path.join(dataset_folder, label)\n",
        "\n",
        "#         # Check if it's a directory (class folder)\n",
        "#         if os.path.isdir(label_path):\n",
        "#             # Get the label (class name) from the folder name\n",
        "#             class_name = label\n",
        "\n",
        "#             # Iterate through the images in the class folder\n",
        "#             for image_file in os.listdir(label_path):\n",
        "#                 image_path = os.path.join(label_path, image_file)\n",
        "\n",
        "#                 # Write the image path and label to the output file\n",
        "#                 f.write(f'{image_path}, {class_name}\\n')\n"
      ],
      "metadata": {
        "id": "3ScHa74gfAXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Dataset"
      ],
      "metadata": {
        "id": "XNma768QgFfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_file_paths = []\n",
        "image_genre_labels = []\n",
        "enc_file_paths = []\n",
        "enc_genre_labels = []\n",
        "\n",
        "labels_file = \"/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train_dataset.txt\"\n",
        "\n",
        "with open(labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.jpg\"):\n",
        "            enc_file_paths.append(filename)\n",
        "            enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.jpg\"):\n",
        "            image_file_paths.append(filename)\n",
        "            image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "OEjnk0W-Pc8Y"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(image_file_paths), len(enc_file_paths)"
      ],
      "metadata": {
        "id": "MmYmWXRJRaXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fd548d2-f3d8-4d81-a5f1-8a1cb54092cd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(68051, 68051)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_file_paths[0:5] , image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "UwLJAYq6dhLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f51d913-6d44-4e63-a917-68250a65f8d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_335_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_834_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_489_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_98_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_592_4.jpg'],\n",
              " ['french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_file_paths[0:5] , enc_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "WxClniG0fpwN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c487c1b3-951e-4081-b2a9-23408b231c7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_124_3.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_37_3.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_690_3.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_240_3.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/train/french_onion_soup/french_onion_soup_835_3.jpg'],\n",
              " ['french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup'])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = set(enc_genre_labels)\n",
        "unique_labels_list = list(unique_labels)\n",
        "\n",
        "print(\"Unique Labels:\", unique_labels_list)"
      ],
      "metadata": {
        "id": "slLNEQBQS-i-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b56922e-690d-4a7d-842c-e74f85655323"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique Labels: ['guacamole', 'chicken_quesadilla', 'french_fries', 'beef_tartare', 'chocolate_cake', 'panna_cotta', 'paella', 'chocolate_mousse', 'peking_duck', 'cheesecake', 'greek_salad', 'breakfast_burrito', 'red_velvet_cake', 'beet_salad', 'lobster_roll_sandwich', 'frozen_yogurt', 'pad_thai', 'oysters', 'escargots', 'prime_rib', 'clam_chowder', 'bread_pudding', 'fried_calamari', 'ravioli', 'gnocchi', 'beignets', 'pulled_pork_sandwich', 'baby_back_ribs', 'pizza', 'bruschetta', 'dumplings', 'chicken_curry', 'nachos', 'creme_brulee', 'lasagna', 'hot_and_sour_soup', 'huevos_rancheros', 'pork_chop', 'samosa', 'caprese_salad', 'grilled_salmon', 'omelette', 'spaghetti_carbonara', 'edamame', 'takoyaki', 'baklava', 'poutine', 'donuts', 'ceviche', 'miso_soup', 'spaghetti_bolognese', 'cannoli', 'sushi', 'sashimi', 'garlic_bread', 'falafel', 'risotto', 'club_sandwich', 'cheese_plate', 'bibimbap', 'fish_and_chips', 'spring_rolls', 'macarons', 'strawberry_shortcake', 'pancakes', 'fried_rice', 'tiramisu', 'hamburger', 'pho', 'foie_gras', 'onion_rings', 'french_toast', 'grilled_cheese_sandwich', 'apple_pie', 'beef_carpaccio', 'macaroni_and_cheese', 'eggs_benedict', 'crab_cakes', 'hot_dog', 'french_onion_soup', 'ice_cream', 'tuna_tartare', 'tacos', 'croque_madame', 'mussels', 'shrimp_and_grits', 'gyoza', 'waffles', 'filet_mignon', 'cup_cakes', 'carrot_cake', 'lobster_bisque', 'seaweed_salad', 'churros', 'steak', 'ramen', 'caesar_salad', 'hummus', 'deviled_eggs', 'chicken_wings', 'scallops']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(unique_labels_list)\n",
        "num_classes"
      ],
      "metadata": {
        "id": "HX2Qa1xQZu6Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a69c2da-7ada-41ee-cc98-aa6c1ab59516"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "percentage = 0.3\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "filtered_enc_file_paths = []\n",
        "filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(enc_file_paths, enc_genre_labels):\n",
        "    if selected_samples_per_label[label] < num_samples_to_include_per_label[label]:\n",
        "        filtered_enc_file_paths.append(path)\n",
        "        filtered_enc_genre_labels.append(label)\n",
        "        selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "Q9xQzx8MV3_U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "_JgyuBkqf45r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a177a598-a0d7-45f4-b6fa-6b31c4a675b0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20365"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "train_file_paths = image_file_paths + filtered_enc_file_paths\n",
        "train_genre_labels = image_genre_labels + filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "2WTpdvsOXRgD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_file_paths)"
      ],
      "metadata": {
        "id": "72g7sZu8gAXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c59a939-8d0d-42eb-9d42-913b056c3bc4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88416"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Dataset"
      ],
      "metadata": {
        "id": "7D26BNoJbJ3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "test_image_file_paths = []\n",
        "test_image_genre_labels = []\n",
        "test_enc_file_paths = []\n",
        "test_enc_genre_labels = []\n",
        "\n",
        "test_labels_file = \"/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test_dataset.txt\"\n",
        "\n",
        "with open(test_labels_file, 'r') as file:\n",
        "    for line in file:\n",
        "        parts = line.strip().split(', ')\n",
        "        filename = parts[0].strip()\n",
        "        labels = parts[1].strip()\n",
        "\n",
        "        if filename.endswith(\"_3.jpg\"):\n",
        "            test_enc_file_paths.append(filename)\n",
        "            test_enc_genre_labels.append(labels)\n",
        "\n",
        "        elif filename.endswith(\"_4.jpg\"):\n",
        "            test_image_file_paths.append(filename)\n",
        "            test_image_genre_labels.append(labels)"
      ],
      "metadata": {
        "id": "LnGxlWzqbLIj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_image_file_paths), len(test_enc_file_paths)"
      ],
      "metadata": {
        "id": "yd9_b3EfbLdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9cca675-75c8-4041-b442-0721be980991"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22716, 22716)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_file_paths[0:5] , test_image_genre_labels[0:5]"
      ],
      "metadata": {
        "id": "Kj5sNm7ngOhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fdfbca-2e1a-4e6c-d518-5dc53e373461"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test/french_onion_soup/french_onion_soup_400_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test/french_onion_soup/french_onion_soup_719_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test/french_onion_soup/french_onion_soup_163_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test/french_onion_soup/french_onion_soup_423_4.jpg',\n",
              "  '/content/MMLearning/data/food-101/flip/multimodal_img_et_flip/test/french_onion_soup/french_onion_soup_243_4.jpg'],\n",
              " ['french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup',\n",
              "  'french_onion_soup'])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "label_counts = {label: 0 for label in unique_labels_list}\n",
        "\n",
        "test_num_samples_to_include_per_label = {}\n",
        "for label in unique_labels_list:\n",
        "    label_count = test_enc_genre_labels.count(label)\n",
        "    num_samples_to_include = int(label_count * percentage)\n",
        "    test_num_samples_to_include_per_label[label] = num_samples_to_include\n",
        "\n",
        "\n",
        "test_filtered_enc_file_paths = []\n",
        "test_filtered_enc_genre_labels = []\n",
        "\n",
        "\n",
        "test_selected_samples_per_label = {label: 0 for label in unique_labels_list}\n",
        "for path, label in zip(test_enc_file_paths, test_enc_genre_labels):\n",
        "    if test_selected_samples_per_label[label] < test_num_samples_to_include_per_label[label]:\n",
        "        test_filtered_enc_file_paths.append(path)\n",
        "        test_filtered_enc_genre_labels.append(label)\n",
        "        test_selected_samples_per_label[label] += 1"
      ],
      "metadata": {
        "id": "rHhvWppqbLxi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_filtered_enc_file_paths)"
      ],
      "metadata": {
        "id": "ZplC_8mxgfng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0de465e2-6ecb-4771-9dce-c227da7da4a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6770"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new lists to combine data from both lists\n",
        "test_file_paths = test_image_file_paths + test_filtered_enc_file_paths\n",
        "test_genre_labels = test_image_genre_labels + test_filtered_enc_genre_labels"
      ],
      "metadata": {
        "id": "C6sjf4Ptbcdf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_file_paths)"
      ],
      "metadata": {
        "id": "T0OH11Z2OJ_k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c47af83-98ee-4b25-e4af-41f824c38ee6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29486"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet"
      ],
      "metadata": {
        "id": "NzBfAyc7apgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, transform=None):\n",
        "        self.file_paths = file_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "        label = int(self.labels[idx])\n",
        "        # label = torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "3Y7VXqUBYf_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_file_paths, train_genre_labels, transform=transform)\n",
        "test_dataset = CustomDataset(test_file_paths, test_genre_labels, transform=transform)"
      ],
      "metadata": {
        "id": "CjVU94XRZOXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32  # Adjust as needed\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "niKEq0PUZVan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "resnet = models.resnet101(pretrained=True)\n",
        "num_features = resnet.fc.in_features\n",
        "resnet.fc = nn.Linear(num_features, num_classes)  # num_classes is the number of classes in your dataset\n",
        "resnet.to(device)\n",
        "print(resnet)\n"
      ],
      "metadata": {
        "id": "WkU78f5UZXqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
        "num_epochs = 30"
      ],
      "metadata": {
        "id": "kQnCFcJCZx5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "6Q79jKkChops"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_loss = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    resnet.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = resnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    training_loss.append(epoch_loss)\n",
        "\n",
        "    print(\"Training Loss==========================>>\")\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} Training Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    resnet.eval()\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = resnet(images)\n",
        "\n",
        "            _, predicted_label = torch.max(outputs, 1)\n",
        "\n",
        "            predicted_classes.extend(predicted_label.cpu().tolist())\n",
        "            actual_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    # precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    # f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    # print(\"Precision:\", precision)\n",
        "    # print(\"Recall:\", recall)\n",
        "    # print(\"F1-score:\", f1)\n",
        "\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    # print(\"Confusion Matrix: \")\n",
        "    # cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    # print(cm)\n",
        "    predicted_classes = np.array(predicted_classes)\n",
        "    actual_labels = np.array(actual_labels)\n",
        "    print(roc_auc_score(actual_labels, predicted_classes))\n",
        "\n",
        "\n",
        "    save_dir = \"/content/Model/Models-Train-30/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(resnet.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "X70EPWQbZ_id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ViT"
      ],
      "metadata": {
        "id": "VmLtbV7uZiUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!pip install transformers evaluate datasets\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "uO3ZsSpNeu9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
        "model = ViTForImageClassification.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "5kwp9qmMZjkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label Encoding"
      ],
      "metadata": {
        "id": "UGs6LJI9guRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "transformed_train_genre_labels = label_encoder.fit_transform(train_genre_labels)"
      ],
      "metadata": {
        "id": "x5qr0pvBcKi0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# label_to_id_mapping = {label: id for label, id in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
        "# label_to_id_mapping"
      ],
      "metadata": {
        "id": "AhfBYrKeYoUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder1 = LabelEncoder()\n",
        "transformed_test_genre_labels = label_encoder1.fit_transform(test_genre_labels)"
      ],
      "metadata": {
        "id": "VJ-m2Q5rgsrj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_to_id_mapping = {label: id for label, id in zip(label_encoder1.classes_, label_encoder1.transform(label_encoder1.classes_))}\n",
        "id_to_label_mapping = {id: label for id, label in enumerate(label_encoder1.classes_)}"
      ],
      "metadata": {
        "id": "jsF-RjLRYbcN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert int64 to int in id_to_label_mapping and label_to_id_mapping\n",
        "id_to_label_mapping = {int(id): label for id, label in id_to_label_mapping.items()}\n",
        "label_to_id_mapping = {label: int(id) for label, id in label_to_id_mapping.items()}"
      ],
      "metadata": {
        "id": "Hnad1bYLakGs"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_label_mapping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUXBAY3Xa3lo",
        "outputId": "1bfdc8f0-e160-4d65-b8e2-183532154ab9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'apple_pie',\n",
              " 1: 'baby_back_ribs',\n",
              " 2: 'baklava',\n",
              " 3: 'beef_carpaccio',\n",
              " 4: 'beef_tartare',\n",
              " 5: 'beet_salad',\n",
              " 6: 'beignets',\n",
              " 7: 'bibimbap',\n",
              " 8: 'bread_pudding',\n",
              " 9: 'breakfast_burrito',\n",
              " 10: 'bruschetta',\n",
              " 11: 'caesar_salad',\n",
              " 12: 'cannoli',\n",
              " 13: 'caprese_salad',\n",
              " 14: 'carrot_cake',\n",
              " 15: 'ceviche',\n",
              " 16: 'cheese_plate',\n",
              " 17: 'cheesecake',\n",
              " 18: 'chicken_curry',\n",
              " 19: 'chicken_quesadilla',\n",
              " 20: 'chicken_wings',\n",
              " 21: 'chocolate_cake',\n",
              " 22: 'chocolate_mousse',\n",
              " 23: 'churros',\n",
              " 24: 'clam_chowder',\n",
              " 25: 'club_sandwich',\n",
              " 26: 'crab_cakes',\n",
              " 27: 'creme_brulee',\n",
              " 28: 'croque_madame',\n",
              " 29: 'cup_cakes',\n",
              " 30: 'deviled_eggs',\n",
              " 31: 'donuts',\n",
              " 32: 'dumplings',\n",
              " 33: 'edamame',\n",
              " 34: 'eggs_benedict',\n",
              " 35: 'escargots',\n",
              " 36: 'falafel',\n",
              " 37: 'filet_mignon',\n",
              " 38: 'fish_and_chips',\n",
              " 39: 'foie_gras',\n",
              " 40: 'french_fries',\n",
              " 41: 'french_onion_soup',\n",
              " 42: 'french_toast',\n",
              " 43: 'fried_calamari',\n",
              " 44: 'fried_rice',\n",
              " 45: 'frozen_yogurt',\n",
              " 46: 'garlic_bread',\n",
              " 47: 'gnocchi',\n",
              " 48: 'greek_salad',\n",
              " 49: 'grilled_cheese_sandwich',\n",
              " 50: 'grilled_salmon',\n",
              " 51: 'guacamole',\n",
              " 52: 'gyoza',\n",
              " 53: 'hamburger',\n",
              " 54: 'hot_and_sour_soup',\n",
              " 55: 'hot_dog',\n",
              " 56: 'huevos_rancheros',\n",
              " 57: 'hummus',\n",
              " 58: 'ice_cream',\n",
              " 59: 'lasagna',\n",
              " 60: 'lobster_bisque',\n",
              " 61: 'lobster_roll_sandwich',\n",
              " 62: 'macaroni_and_cheese',\n",
              " 63: 'macarons',\n",
              " 64: 'miso_soup',\n",
              " 65: 'mussels',\n",
              " 66: 'nachos',\n",
              " 67: 'omelette',\n",
              " 68: 'onion_rings',\n",
              " 69: 'oysters',\n",
              " 70: 'pad_thai',\n",
              " 71: 'paella',\n",
              " 72: 'pancakes',\n",
              " 73: 'panna_cotta',\n",
              " 74: 'peking_duck',\n",
              " 75: 'pho',\n",
              " 76: 'pizza',\n",
              " 77: 'pork_chop',\n",
              " 78: 'poutine',\n",
              " 79: 'prime_rib',\n",
              " 80: 'pulled_pork_sandwich',\n",
              " 81: 'ramen',\n",
              " 82: 'ravioli',\n",
              " 83: 'red_velvet_cake',\n",
              " 84: 'risotto',\n",
              " 85: 'samosa',\n",
              " 86: 'sashimi',\n",
              " 87: 'scallops',\n",
              " 88: 'seaweed_salad',\n",
              " 89: 'shrimp_and_grits',\n",
              " 90: 'spaghetti_bolognese',\n",
              " 91: 'spaghetti_carbonara',\n",
              " 92: 'spring_rolls',\n",
              " 93: 'steak',\n",
              " 94: 'strawberry_shortcake',\n",
              " 95: 'sushi',\n",
              " 96: 'tacos',\n",
              " 97: 'takoyaki',\n",
              " 98: 'tiramisu',\n",
              " 99: 'tuna_tartare',\n",
              " 100: 'waffles'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_genre_labels), len(transformed_train_genre_labels)"
      ],
      "metadata": {
        "id": "Q8iTY22HfXsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6988c0b-7605-49b4-d1ff-06e4559330f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88416, 88416)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_genre_labels), len(transformed_test_genre_labels)"
      ],
      "metadata": {
        "id": "ClRZgMkgg4Sw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbf27a45-e8b2-45eb-ee1a-15e87a61a68a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29486, 29486)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Creating"
      ],
      "metadata": {
        "id": "e297rXtbgwdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_data = {'image': train_file_paths, 'label': transformed_train_genre_labels}\n",
        "ds_train = Dataset.from_dict(train_data)"
      ],
      "metadata": {
        "id": "6XKv_SEBa6Sz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = {'image': test_file_paths, 'label': transformed_test_genre_labels}\n",
        "ds_val = Dataset.from_dict(val_data)"
      ],
      "metadata": {
        "id": "djhbA0nCaDgq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = unique_labels_list\n",
        "labels"
      ],
      "metadata": {
        "id": "5TGQqMVrbMW3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532ac8c4-0bf4-49e9-e3c0-436d527ed206"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['guacamole',\n",
              " 'chicken_quesadilla',\n",
              " 'french_fries',\n",
              " 'beef_tartare',\n",
              " 'chocolate_cake',\n",
              " 'panna_cotta',\n",
              " 'paella',\n",
              " 'chocolate_mousse',\n",
              " 'peking_duck',\n",
              " 'cheesecake',\n",
              " 'greek_salad',\n",
              " 'breakfast_burrito',\n",
              " 'red_velvet_cake',\n",
              " 'beet_salad',\n",
              " 'lobster_roll_sandwich',\n",
              " 'frozen_yogurt',\n",
              " 'pad_thai',\n",
              " 'oysters',\n",
              " 'escargots',\n",
              " 'prime_rib',\n",
              " 'clam_chowder',\n",
              " 'bread_pudding',\n",
              " 'fried_calamari',\n",
              " 'ravioli',\n",
              " 'gnocchi',\n",
              " 'beignets',\n",
              " 'pulled_pork_sandwich',\n",
              " 'baby_back_ribs',\n",
              " 'pizza',\n",
              " 'bruschetta',\n",
              " 'dumplings',\n",
              " 'chicken_curry',\n",
              " 'nachos',\n",
              " 'creme_brulee',\n",
              " 'lasagna',\n",
              " 'hot_and_sour_soup',\n",
              " 'huevos_rancheros',\n",
              " 'pork_chop',\n",
              " 'samosa',\n",
              " 'caprese_salad',\n",
              " 'grilled_salmon',\n",
              " 'omelette',\n",
              " 'spaghetti_carbonara',\n",
              " 'edamame',\n",
              " 'takoyaki',\n",
              " 'baklava',\n",
              " 'poutine',\n",
              " 'donuts',\n",
              " 'ceviche',\n",
              " 'miso_soup',\n",
              " 'spaghetti_bolognese',\n",
              " 'cannoli',\n",
              " 'sushi',\n",
              " 'sashimi',\n",
              " 'garlic_bread',\n",
              " 'falafel',\n",
              " 'risotto',\n",
              " 'club_sandwich',\n",
              " 'cheese_plate',\n",
              " 'bibimbap',\n",
              " 'fish_and_chips',\n",
              " 'spring_rolls',\n",
              " 'macarons',\n",
              " 'strawberry_shortcake',\n",
              " 'pancakes',\n",
              " 'fried_rice',\n",
              " 'tiramisu',\n",
              " 'hamburger',\n",
              " 'pho',\n",
              " 'foie_gras',\n",
              " 'onion_rings',\n",
              " 'french_toast',\n",
              " 'grilled_cheese_sandwich',\n",
              " 'apple_pie',\n",
              " 'beef_carpaccio',\n",
              " 'macaroni_and_cheese',\n",
              " 'eggs_benedict',\n",
              " 'crab_cakes',\n",
              " 'hot_dog',\n",
              " 'french_onion_soup',\n",
              " 'ice_cream',\n",
              " 'tuna_tartare',\n",
              " 'tacos',\n",
              " 'croque_madame',\n",
              " 'mussels',\n",
              " 'shrimp_and_grits',\n",
              " 'gyoza',\n",
              " 'waffles',\n",
              " 'filet_mignon',\n",
              " 'cup_cakes',\n",
              " 'carrot_cake',\n",
              " 'lobster_bisque',\n",
              " 'seaweed_salad',\n",
              " 'churros',\n",
              " 'steak',\n",
              " 'ramen',\n",
              " 'caesar_salad',\n",
              " 'hummus',\n",
              " 'deviled_eggs',\n",
              " 'chicken_wings',\n",
              " 'scallops']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL.Image as pil\n",
        "\n",
        "def transform(examples):\n",
        "  inputs = image_processor([pil.open(img).convert(\"RGB\") for img in examples[\"image\"]], return_tensors=\"pt\")\n",
        "  inputs[\"labels\"] = examples[\"label\"]\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "9dEy0tTcbTfJ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ds_train.with_transform(transform)\n",
        "val_dataset = ds_val.with_transform(transform)"
      ],
      "metadata": {
        "id": "owz6NIGTbVA7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "pet0z7BJdDZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00a21e2e-1fd5-48ff-e9bf-f7d68108f0b4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 88416\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset"
      ],
      "metadata": {
        "id": "00nnqJFvdHIj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "058426a4-c111-4fc6-b4af-c12a1b596b91"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['image', 'label'],\n",
              "    num_rows: 29486\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in val_dataset:\n",
        "  print(item['pixel_values'].shape)\n",
        "  print(item[\"labels\"])\n",
        "  break"
      ],
      "metadata": {
        "id": "sPsIroeXbjlY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8548c500-98b1-4e32-d88a-b7883984d5dd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def collate_fn(batch):\n",
        "  return {\n",
        "      \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch]),\n",
        "      \"labels\": torch.tensor([x[\"labels\"] for x in batch]),\n",
        "  }"
      ],
      "metadata": {
        "id": "22a48uX4bnZ0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the ViT model\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(unique_labels_list),\n",
        "    # id2label={str(i): c for i, c in enumerate(unique_labels_list)},\n",
        "    id2label=id_to_label_mapping,\n",
        "    label2id=label_to_id_mapping,\n",
        "    # label2id={c: str(i) for i, c in enumerate(unique_labels_list)},\n",
        "    ignore_mismatched_sizes=True,\n",
        ")"
      ],
      "metadata": {
        "id": "sIQenfhHbz_N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dea5d73-3a04-4fa7-d1da-6c47c7da38f2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/config.json\n",
            "Model config ViTConfig {\n",
            "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
            "  \"architectures\": [\n",
            "    \"ViTForImageClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"encoder_stride\": 16,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"apple_pie\",\n",
            "    \"1\": \"baby_back_ribs\",\n",
            "    \"2\": \"baklava\",\n",
            "    \"3\": \"beef_carpaccio\",\n",
            "    \"4\": \"beef_tartare\",\n",
            "    \"5\": \"beet_salad\",\n",
            "    \"6\": \"beignets\",\n",
            "    \"7\": \"bibimbap\",\n",
            "    \"8\": \"bread_pudding\",\n",
            "    \"9\": \"breakfast_burrito\",\n",
            "    \"10\": \"bruschetta\",\n",
            "    \"11\": \"caesar_salad\",\n",
            "    \"12\": \"cannoli\",\n",
            "    \"13\": \"caprese_salad\",\n",
            "    \"14\": \"carrot_cake\",\n",
            "    \"15\": \"ceviche\",\n",
            "    \"16\": \"cheese_plate\",\n",
            "    \"17\": \"cheesecake\",\n",
            "    \"18\": \"chicken_curry\",\n",
            "    \"19\": \"chicken_quesadilla\",\n",
            "    \"20\": \"chicken_wings\",\n",
            "    \"21\": \"chocolate_cake\",\n",
            "    \"22\": \"chocolate_mousse\",\n",
            "    \"23\": \"churros\",\n",
            "    \"24\": \"clam_chowder\",\n",
            "    \"25\": \"club_sandwich\",\n",
            "    \"26\": \"crab_cakes\",\n",
            "    \"27\": \"creme_brulee\",\n",
            "    \"28\": \"croque_madame\",\n",
            "    \"29\": \"cup_cakes\",\n",
            "    \"30\": \"deviled_eggs\",\n",
            "    \"31\": \"donuts\",\n",
            "    \"32\": \"dumplings\",\n",
            "    \"33\": \"edamame\",\n",
            "    \"34\": \"eggs_benedict\",\n",
            "    \"35\": \"escargots\",\n",
            "    \"36\": \"falafel\",\n",
            "    \"37\": \"filet_mignon\",\n",
            "    \"38\": \"fish_and_chips\",\n",
            "    \"39\": \"foie_gras\",\n",
            "    \"40\": \"french_fries\",\n",
            "    \"41\": \"french_onion_soup\",\n",
            "    \"42\": \"french_toast\",\n",
            "    \"43\": \"fried_calamari\",\n",
            "    \"44\": \"fried_rice\",\n",
            "    \"45\": \"frozen_yogurt\",\n",
            "    \"46\": \"garlic_bread\",\n",
            "    \"47\": \"gnocchi\",\n",
            "    \"48\": \"greek_salad\",\n",
            "    \"49\": \"grilled_cheese_sandwich\",\n",
            "    \"50\": \"grilled_salmon\",\n",
            "    \"51\": \"guacamole\",\n",
            "    \"52\": \"gyoza\",\n",
            "    \"53\": \"hamburger\",\n",
            "    \"54\": \"hot_and_sour_soup\",\n",
            "    \"55\": \"hot_dog\",\n",
            "    \"56\": \"huevos_rancheros\",\n",
            "    \"57\": \"hummus\",\n",
            "    \"58\": \"ice_cream\",\n",
            "    \"59\": \"lasagna\",\n",
            "    \"60\": \"lobster_bisque\",\n",
            "    \"61\": \"lobster_roll_sandwich\",\n",
            "    \"62\": \"macaroni_and_cheese\",\n",
            "    \"63\": \"macarons\",\n",
            "    \"64\": \"miso_soup\",\n",
            "    \"65\": \"mussels\",\n",
            "    \"66\": \"nachos\",\n",
            "    \"67\": \"omelette\",\n",
            "    \"68\": \"onion_rings\",\n",
            "    \"69\": \"oysters\",\n",
            "    \"70\": \"pad_thai\",\n",
            "    \"71\": \"paella\",\n",
            "    \"72\": \"pancakes\",\n",
            "    \"73\": \"panna_cotta\",\n",
            "    \"74\": \"peking_duck\",\n",
            "    \"75\": \"pho\",\n",
            "    \"76\": \"pizza\",\n",
            "    \"77\": \"pork_chop\",\n",
            "    \"78\": \"poutine\",\n",
            "    \"79\": \"prime_rib\",\n",
            "    \"80\": \"pulled_pork_sandwich\",\n",
            "    \"81\": \"ramen\",\n",
            "    \"82\": \"ravioli\",\n",
            "    \"83\": \"red_velvet_cake\",\n",
            "    \"84\": \"risotto\",\n",
            "    \"85\": \"samosa\",\n",
            "    \"86\": \"sashimi\",\n",
            "    \"87\": \"scallops\",\n",
            "    \"88\": \"seaweed_salad\",\n",
            "    \"89\": \"shrimp_and_grits\",\n",
            "    \"90\": \"spaghetti_bolognese\",\n",
            "    \"91\": \"spaghetti_carbonara\",\n",
            "    \"92\": \"spring_rolls\",\n",
            "    \"93\": \"steak\",\n",
            "    \"94\": \"strawberry_shortcake\",\n",
            "    \"95\": \"sushi\",\n",
            "    \"96\": \"tacos\",\n",
            "    \"97\": \"takoyaki\",\n",
            "    \"98\": \"tiramisu\",\n",
            "    \"99\": \"tuna_tartare\",\n",
            "    \"100\": \"waffles\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"apple_pie\": 0,\n",
            "    \"baby_back_ribs\": 1,\n",
            "    \"baklava\": 2,\n",
            "    \"beef_carpaccio\": 3,\n",
            "    \"beef_tartare\": 4,\n",
            "    \"beet_salad\": 5,\n",
            "    \"beignets\": 6,\n",
            "    \"bibimbap\": 7,\n",
            "    \"bread_pudding\": 8,\n",
            "    \"breakfast_burrito\": 9,\n",
            "    \"bruschetta\": 10,\n",
            "    \"caesar_salad\": 11,\n",
            "    \"cannoli\": 12,\n",
            "    \"caprese_salad\": 13,\n",
            "    \"carrot_cake\": 14,\n",
            "    \"ceviche\": 15,\n",
            "    \"cheese_plate\": 16,\n",
            "    \"cheesecake\": 17,\n",
            "    \"chicken_curry\": 18,\n",
            "    \"chicken_quesadilla\": 19,\n",
            "    \"chicken_wings\": 20,\n",
            "    \"chocolate_cake\": 21,\n",
            "    \"chocolate_mousse\": 22,\n",
            "    \"churros\": 23,\n",
            "    \"clam_chowder\": 24,\n",
            "    \"club_sandwich\": 25,\n",
            "    \"crab_cakes\": 26,\n",
            "    \"creme_brulee\": 27,\n",
            "    \"croque_madame\": 28,\n",
            "    \"cup_cakes\": 29,\n",
            "    \"deviled_eggs\": 30,\n",
            "    \"donuts\": 31,\n",
            "    \"dumplings\": 32,\n",
            "    \"edamame\": 33,\n",
            "    \"eggs_benedict\": 34,\n",
            "    \"escargots\": 35,\n",
            "    \"falafel\": 36,\n",
            "    \"filet_mignon\": 37,\n",
            "    \"fish_and_chips\": 38,\n",
            "    \"foie_gras\": 39,\n",
            "    \"french_fries\": 40,\n",
            "    \"french_onion_soup\": 41,\n",
            "    \"french_toast\": 42,\n",
            "    \"fried_calamari\": 43,\n",
            "    \"fried_rice\": 44,\n",
            "    \"frozen_yogurt\": 45,\n",
            "    \"garlic_bread\": 46,\n",
            "    \"gnocchi\": 47,\n",
            "    \"greek_salad\": 48,\n",
            "    \"grilled_cheese_sandwich\": 49,\n",
            "    \"grilled_salmon\": 50,\n",
            "    \"guacamole\": 51,\n",
            "    \"gyoza\": 52,\n",
            "    \"hamburger\": 53,\n",
            "    \"hot_and_sour_soup\": 54,\n",
            "    \"hot_dog\": 55,\n",
            "    \"huevos_rancheros\": 56,\n",
            "    \"hummus\": 57,\n",
            "    \"ice_cream\": 58,\n",
            "    \"lasagna\": 59,\n",
            "    \"lobster_bisque\": 60,\n",
            "    \"lobster_roll_sandwich\": 61,\n",
            "    \"macaroni_and_cheese\": 62,\n",
            "    \"macarons\": 63,\n",
            "    \"miso_soup\": 64,\n",
            "    \"mussels\": 65,\n",
            "    \"nachos\": 66,\n",
            "    \"omelette\": 67,\n",
            "    \"onion_rings\": 68,\n",
            "    \"oysters\": 69,\n",
            "    \"pad_thai\": 70,\n",
            "    \"paella\": 71,\n",
            "    \"pancakes\": 72,\n",
            "    \"panna_cotta\": 73,\n",
            "    \"peking_duck\": 74,\n",
            "    \"pho\": 75,\n",
            "    \"pizza\": 76,\n",
            "    \"pork_chop\": 77,\n",
            "    \"poutine\": 78,\n",
            "    \"prime_rib\": 79,\n",
            "    \"pulled_pork_sandwich\": 80,\n",
            "    \"ramen\": 81,\n",
            "    \"ravioli\": 82,\n",
            "    \"red_velvet_cake\": 83,\n",
            "    \"risotto\": 84,\n",
            "    \"samosa\": 85,\n",
            "    \"sashimi\": 86,\n",
            "    \"scallops\": 87,\n",
            "    \"seaweed_salad\": 88,\n",
            "    \"shrimp_and_grits\": 89,\n",
            "    \"spaghetti_bolognese\": 90,\n",
            "    \"spaghetti_carbonara\": 91,\n",
            "    \"spring_rolls\": 92,\n",
            "    \"steak\": 93,\n",
            "    \"strawberry_shortcake\": 94,\n",
            "    \"sushi\": 95,\n",
            "    \"tacos\": 96,\n",
            "    \"takoyaki\": 97,\n",
            "    \"tiramisu\": 98,\n",
            "    \"tuna_tartare\": 99,\n",
            "    \"waffles\": 100\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"model_type\": \"vit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"patch_size\": 16,\n",
            "  \"qkv_bias\": true,\n",
            "  \"transformers_version\": \"4.32.1\"\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--vit-base-patch16-224/snapshots/3f49326eb077187dfe1c2a2bb15fbd74e6ab91e3/model.safetensors\n",
            "All model checkpoint weights were used when initializing ViTForImageClassification.\n",
            "\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([101]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([101, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation Metric"
      ],
      "metadata": {
        "id": "KLVUJAqgcCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "\n",
        "  # auroc_score = roc_auc_score(eval_pred.label_ids, np.argmax(eval_pred.predictions, axis=1))\n",
        "  # print(f\"AUROC Score: {auroc_score:.4f}\")\n",
        "\n",
        "  return {**accuracy_score, **f1_score}"
      ],
      "metadata": {
        "id": "23DkFdTNcCtE"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "FuDfvAopcHUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install transformers[torch]"
      ],
      "metadata": {
        "id": "665OJ9i4cudJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"/content/MMLearning/data/Models/Model-13\", # output directory\n",
        "  per_device_train_batch_size=32, # batch size per device during training\n",
        "  evaluation_strategy=\"steps\",    # evaluation strategy to adopt during training\n",
        "  num_train_epochs=40,             # total number of training epochs\n",
        "  # fp16=True,                    # use mixed precision\n",
        "  save_steps=9000,                # number of update steps before saving checkpoint\n",
        "  eval_steps=9000,                # number of update steps before evaluating\n",
        "  logging_steps=9000,             # number of update steps before logging\n",
        "  # save_steps=50,\n",
        "  # eval_steps=50,\n",
        "  # logging_steps=50,\n",
        "  save_total_limit=6,             # limit the total amount of checkpoints on disk\n",
        "  remove_unused_columns=False,    # remove unused columns from the dataset\n",
        "  push_to_hub=False,              # do not push the model to the hub\n",
        "  report_to='tensorboard',        # report metrics to tensorboard\n",
        "  load_best_model_at_end=True,    # load the best model at the end of training\n",
        ")"
      ],
      "metadata": {
        "id": "xApZuICNcqvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "971a709f-aa3a-48ce-9594-8680e1c512e1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                        # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                 # training arguments, defined above\n",
        "    data_collator=collate_fn,           # the data collator that will be used for batching\n",
        "    compute_metrics=compute_metrics,    # the metrics function that will be used for evaluation\n",
        "    train_dataset=train_dataset,        # training dataset\n",
        "    eval_dataset=val_dataset,           # evaluation dataset\n",
        "    tokenizer=image_processor,          # the processor that will be used for preprocessing the images\n",
        ")"
      ],
      "metadata": {
        "id": "CDA6IEUzc6M8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "0CI-Z4G_c9WT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "f59cd335-2392-4f05-bc82-55e75e2bdcca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 88,416\n",
            "  Num Epochs = 40\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 110,520\n",
            "  Number of trainable parameters = 85,876,325\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27001' max='110520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 27001/110520 10:59:44 < 34:00:50, 0.68 it/s, Epoch 9.77/40]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.083000</td>\n",
              "      <td>1.230797</td>\n",
              "      <td>0.739063</td>\n",
              "      <td>0.749550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.205800</td>\n",
              "      <td>1.383399</td>\n",
              "      <td>0.749983</td>\n",
              "      <td>0.756967</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='775' max='3686' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 775/3686 01:50 < 06:54, 7.03 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 29486\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/MMLearning/data/Models/Model-13/checkpoint-9000\n",
            "Configuration saved in /content/MMLearning/data/Models/Model-13/checkpoint-9000/config.json\n",
            "Model weights saved in /content/MMLearning/data/Models/Model-13/checkpoint-9000/pytorch_model.bin\n",
            "Image processor saved in /content/MMLearning/data/Models/Model-13/checkpoint-9000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 29486\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/MMLearning/data/Models/Model-13/checkpoint-18000\n",
            "Configuration saved in /content/MMLearning/data/Models/Model-13/checkpoint-18000/config.json\n",
            "Model weights saved in /content/MMLearning/data/Models/Model-13/checkpoint-18000/pytorch_model.bin\n",
            "Image processor saved in /content/MMLearning/data/Models/Model-13/checkpoint-18000/preprocessor_config.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 29486\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Model Selection"
      ],
      "metadata": {
        "id": "LWIDbmsuZ6yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "valid_dataset_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "j6YOll-WZ7RK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = 9000"
      ],
      "metadata": {
        "id": "b_DhOKQqcRsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "\n",
        "while checkpoint <= 45000:\n",
        "    model = ViTForImageClassification.from_pretrained(f\"/content/Model/Models-Train-28/checkpoint-{checkpoint}\").to(device)\n",
        "    image_processor = ViTImageProcessor.from_pretrained(f\"/content/Model/Models-Train-28/checkpoint-{checkpoint}\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    predictions, labels = [], []\n",
        "\n",
        "    for batch in valid_dataset_loader:\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        label_ids = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=label_ids)\n",
        "        logits = outputs.logits.detach().cpu()\n",
        "\n",
        "        predictions.extend(logits.argmax(dim=-1).tolist())\n",
        "        labels.extend(label_ids.tolist())\n",
        "\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    precision = precision_score(labels, predictions, average='weighted')\n",
        "    recall = recall_score(labels, predictions, average='weighted')\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "\n",
        "    print(\"Accuracy: \", accuracy)\n",
        "    print(\"precision: \", precision)\n",
        "    print(\"f1_score: \", f1)\n",
        "    print(\"recall\", recall)\n",
        "    print(classification_report(labels, predictions))\n",
        "    checkpoint = checkpoint + 9000"
      ],
      "metadata": {
        "id": "OLAwNwLPbe8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}